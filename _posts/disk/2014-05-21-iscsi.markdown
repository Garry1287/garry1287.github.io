---
layout: post
title:  "iscsi"
date:   2014-05-21 09:13:17 +0400
categories: iscsi
tags: disk
---

# iscsi

[http://www.ibm.com/developerworks/ru/library/l-virt/](http://www.ibm.com/developerworks/ru/library/l-virt/)



[http://www.video-ip.ru/support/article-iscsi-4.htm](http://www.video-ip.ru/support/article-iscsi-4.htm)
[http://www.opennet.ru/tips/info/2037.shtml](http://www.opennet.ru/tips/info/2037.shtml)
[http://xgu.ru/wiki/iSCSI](http://xgu.ru/wiki/iSCSI)
[http://www.nixp.ru/news/%D0%9F%D1%80%D0%BE%D0%B5%D0%BA%D1%82%D1%8B-Open-iSCSI-%D0%B8-Linux-iSCSI-%D0%BE%D0%B1%D1%8A%D0%B5%D0%B4%D0%B8%D0%BD%D1%8F%D1%8E%D1%82%D1%81%D1%8F.html](http://www.nixp.ru/news/%D0%9F%D1%80%D0%BE%D0%B5%D0%BA%D1%82%D1%8B-Open-iSCSI-%D0%B8-Linux-iSCSI-%D0%BE%D0%B1%D1%8A%D0%B5%D0%B4%D0%B8%D0%BD%D1%8F%D1%8E%D1%82%D1%81%D1%8F.html)
[http://www.bog.pp.ru/hard/scsi.html](http://www.bog.pp.ru/hard/scsi.html)
[http://www.opennet.ru/base/sys/iscsi_fedora.txt.html](http://www.opennet.ru/base/sys/iscsi_fedora.txt.html)
[http://armanenshaft-linux.blogspot.ru/2009/04/iscsi-ubuntu-linux.html](http://armanenshaft-linux.blogspot.ru/2009/04/iscsi-ubuntu-linux.html)
[http://udatov.blogspot.ru/2010_05_01_archive.html](http://udatov.blogspot.ru/2010_05_01_archive.html)
[http://www.cuddletech.com/articles/iscsi/index.html](http://www.cuddletech.com/articles/iscsi/index.html)
[http://habrahabr.ru/post/97529/](http://habrahabr.ru/post/97529/)
storusint.com/articles/ipsan_iscsi.htm


192.168.1.44	Сервер для управления esxi, c которого цепляемся на 192.168.200.200 (esxi) 
192.168.200.200(esxi)-192.168.200.201(debian) это сеть для iscsi

На самом дебиане сделан raid из 2 дисков по ТБ, рейд не программный, а железный
Контроллер для рейда интеловский

root@NAS1:/proc/sys/dev/scsi# cat /etc/scst.conf
#HANDLER vdisk_blockio {
#       DEVICE disk01 {
#               filename /dev/sda4
#       }
#}

HANDLER vdisk_fileio {
        DEVICE disk01 {
                filename /dev/sda4
                nv_cache 1
        }
}

TARGET_DRIVER iscsi {
        enabled 1

        TARGET iqn.2006-10.net.vlnb:tgt {
                LUN 0 disk01
#               LUN 1 disk02

                allowed_portal 192.168.200.201
#               allowed_portal 192.168.121.22
                enabled 1
        }
}


Получается iqn.2006-10.net.vlnb:tgt  - это IQN для хранилища
LUN 0 - это раздел /dev/sda4, на котором развёрнут lvm
Только в этом lvm нихрена нету. Никаких pv и vg и lv
Во всяком случае 

fdisk -l /dev/sda

Disk /dev/sda: 999.0 GB, 998999326720 bytes
255 heads, 63 sectors/track, 121454 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0007a30f

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1               1          61      487424   83  Linux
Partition 1 does not end on cylinder boundary.
/dev/sda2              61         426     2929664   83  Linux
Partition 2 does not end on cylinder boundary.
/dev/sda3             426         548      976896   82  Linux swap / Solaris
Partition 3 does not end on cylinder boundary.
/dev/sda4             548      121455   971189248   8e  Linux LVM




tcp        0      0 0.0.0.0:3260            0.0.0.0:*               LISTEN      1489/iscsi-scstd
tcp6       0      0 :::3260                 :::*                    LISTEN      1489/iscsi-scstd








В случае NAS данные хранятся на неком сервере (часто с локально подключённым большим массивом дисков) 
и в сеть другим компьютерам они предоставляются в виде файлов по прикладным протоколам (SMB/CIFS, NFS, FTP, SFTP, HTTP, WebDAV, Direct Connect, BitTorrent и др.)

В случае SAN есть некий компьютер с большим дисковым массивом, но чаще даже не компьютер, а отдельная дисковая полка или целая дисковая стойка (сторадж). 
Дисковый объём этого хранилища нарезается на логические единицы LUN (Logical Unit Number) и в сеть хранения данных клиентам предоставляется (презентуется)
 именно LUN'ы (т.е. куски дискового пространства).
Создание в этом дисковом пространстве, предоставленном хранилищем, дисковых разделов, форматированием их в некую ФС и 
размещением там файлов занимается уже тот сервер, которому был презентован этот LUN. Само хранилище знает только о LUN'ах, и не знает ничего о более высокоуровневых логических структурах 
на этом диске (типа файловых систем и файлов).

Т.е. в NAS в сеть предоставляются файлы по высокоуровневым прикладным протоколам, а в SAN в сеть предоставляются диски (блочные устройства) 
в виде логических дисковых единиц LUN по более низкоуровневому протоколу дискового обмена (SCSI).

Так вот в случае SAN получается, что дисковое хранилище презентует куски своего дискового пространства множеству серверов. 
А каждый такой сервер по сети хранения данных «общается» с презентованным ему диском по протоколу SCSI.
И есть несколько вариантов среды для связи сервера, используещего диск, с дисковым хранилищем, презентующим диск.

Первый вариант:
1) Fibre Channel (FC)
en.wikipedia.org/wiki/Fibre_Channel
Это оптический канал. Если серверы и дисковое хранилище расположены в соседних стойках, то вся Fibre Channel инфраструктура может ограничиваться оптическими патчкордами,
 соединяющими FC-адаптеры сервера и дискового хранилища (впрочем, в этом случае хватило бы даже SCSI-кабеля).
В действительности же в организациях с большой IT-инфраструкутрой в SAN обычно входят множество дисковых стоек, множество оптических патч-кордов, 
множество FC-свитчей и FC-коммутаторов, множество магистральных оптических каналов. Как правило, все звенья FC-маршрута от сервера до хранилища ещё и дублируются
 (два FC-адаптера, два оптических патч-корда, два FC-коммутатора и т.д.), чтобы сервер имел доступ к диску по нескольким независимым путям (multipathing), 
чтобы обеспечивать резервирование в случае обрыва одного из маршрутов



В этом случае команды протокола SCSI между сервером и дисковым хранилищем передаются в среде обычной Ethernet-сети, 
т.е. через обычный сетевой Ethernet-адаптер и обычную витую пару. Т.е. можно сказать, что iSCSI — это SCSI over Ethernet 
(а точнее даже SCSI over TCP/IP).
Для подключения дисков через iSCSI можно, конечно, использовать тот же сетевой интерфейс, который сервер использует для обычного сетевого обмена,
 но корпоративная сеть обычно и так заметно утилизирована различным обменом данными между серверами и клиентами, 
и это будет замедлять работу с диском по сети. Поэтому для организации инфраструктуры iSCSI обычно разворачивают отдельную гигабитную сеть,
 немаршрутизируемую с корпоративной Ethernet-сетью. Т.е. в серверах для доступа к дискам по iSCSI используется отдельный сетевой интерфейс,
 отдельные сетевые кабели, отдельные свитчи и маршрутизаторы, т.е. это физически отдельная сеть.

Но даже с учётом построения отдельной Ethernet-сети решение построения SAN на iSCSI гораздо дешевле, чем на оптике 
(можете посмотреть в прайсах стоимость FC-адаптеров/кабелей/свитчей/маршрутизаторов). iSCSI — это, что назвается, 
вариант реализации SAN для бедных. По скоростям дискового обмена он проигрывает оптике, 
но для относительно небольших компаний это вполне приемлемый вариант организации дискового хранилища и удалённого подключения 
с него дисков к множеству серверов.

Кроме оптики и витой пары раньше ещё можно было встретить реализацию SAN на медных кабелях, но такого вроде уже не встретить,
 поэтому другие варианты среды передачи в SAN рассматривать не буду.

Для презентования внутри SAN определённого диска (LUN'а) с дискового хранилища не всем подряд, а только определённому серверу, 
обычно используют некую привязку. При использовании Fibre Channel это может быть привязка к WWN (World Wide Name) — 
восьмибайтовый уникальный 16-ричный аппаратный номер FC-интерфейса (аналог MAC-адреса у Ethernet-интерфейсов). 
В случае использования iSCSI это может быть MAC-адрес или IP-адрес. Также дополнительно может быть парольная защита,
 чтобы кто угодно внутри этой сети не смог подключить определённый диск.

Так вот, возвращаясь к iSCSI… В клиент-серверной терминологии протокола iSCSI (впрочем, как и самого SCSI) есть такие понятия, 
как iSCSI Initiator и iSCSI Target.
iSCSI Initiator — это тот сервер, который подключает к себе по сети диск, презентованный ему с дискового хранилища.
А iSCSI Target — это соответственно сторона, предоставляющая диск (обычно это тот сервер, к которому этот диск/массив подключён фисически, 
например, через SATA/SCSI-контроллер). Любой сервер по отношению к какому-то таргету может выступать iSCSI Initiator'ом, 
и одновременно для других он сам может выступать в роли iSCSI Target, предоставляя в сеть свои дисковые ресурсы по iSCSI.

Собственно, в данном топике и представлен вариант Linux-дистрибутива (iSCSI Target Box), который можно установить на компьютер 
с кучей вместительных локальных жёстких дисков, чтобы с него презентовать эти диски другим компьютерам в сети по протоколу iSCSI 
(т.е. организовать там iSCSI Target). 







Iscsi multipath

[http://ivirt-it.ru/2013/08/multipath-io-iscsi/](http://ivirt-it.ru/2013/08/multipath-io-iscsi/)

